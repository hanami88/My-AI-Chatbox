import os
from dotenv import load_dotenv
from langchain_groq import ChatGroq
from langchain.document_loaders import (
    TextLoader,
    PyPDFLoader,
    Docx2txtLoader,
    DirectoryLoader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from config import CUSTOM_SYSTEM_PROMPT, RAG_PROMPT_TEMPLATE, MODEL_CONFIG, VECTOR_CONFIG

load_dotenv()


class RAGChatbot:
    def __init__(self):
        self.llm = None
        self.vectorstore = None
        self.qa_chain = None
        self.setup_components()

    def setup_components(self):
        """Kh·ªüi t·∫°o c√°c component c·ªßa RAG"""
        print("üîÑ ƒêang kh·ªüi t·∫°o AI chatbot...")

        # 1. Kh·ªüi t·∫°o LLM
        self.llm = ChatGroq(
            groq_api_key=os.getenv("GROQ_API_KEY"),
            **MODEL_CONFIG
        )

        # 2. Load documents
        self.load_documents()

        # 3. Setup retrieval QA chain
        self.setup_qa_chain()

        print("‚úÖ AI chatbot ƒë√£ s·∫µn s√†ng!")

    def load_documents(self):
        """Load v√† x·ª≠ l√Ω t√†i li·ªáu"""
        print("üìö ƒêang ƒë·ªçc t√†i li·ªáu...")

        documents_path = "documents"
        if not os.path.exists(documents_path):
            os.makedirs(documents_path)
            print(f"üìÅ ƒê√£ t·∫°o th∆∞ m·ª•c {documents_path}")
            print("üí° H√£y th√™m t√†i li·ªáu (.txt, .pdf, .docx) v√†o th∆∞ m·ª•c n√†y!")
            return

        # Load c√°c lo·∫°i file kh√°c nhau
        loaders = []

        # Text files
        if any(f.endswith('.txt') for f in os.listdir(documents_path)):
            txt_loader = DirectoryLoader(
                documents_path,
                glob="*.txt",
                loader_cls=TextLoader
            )
            loaders.append(txt_loader)

        # PDF files
        pdf_files = [f for f in os.listdir(documents_path) if f.endswith('.pdf')]
        for pdf_file in pdf_files:
            pdf_loader = PyPDFLoader(os.path.join(documents_path, pdf_file))
            loaders.append(pdf_loader)

        # DOCX files
        docx_files = [f for f in os.listdir(documents_path) if f.endswith('.docx')]
        for docx_file in docx_files:
            docx_loader = Docx2txtLoader(os.path.join(documents_path, docx_file))
            loaders.append(docx_loader)

        # Load t·∫•t c·∫£ documents
        all_documents = []
        for loader in loaders:
            try:
                docs = loader.load()
                all_documents.extend(docs)
            except Exception as e:
                print(f"‚ùå L·ªói khi ƒë·ªçc t√†i li·ªáu: {e}")

        if not all_documents:
            print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y t√†i li·ªáu n√†o!")
            return

        # Split documents
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=VECTOR_CONFIG["chunk_size"],
            chunk_overlap=VECTOR_CONFIG["chunk_overlap"]
        )
        texts = text_splitter.split_documents(all_documents)

        # Create embeddings
        embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2"
        )

        # Create vector store
        self.vectorstore = Chroma.from_documents(
            documents=texts,
            embedding=embeddings,
            persist_directory="vector_store"
        )

        print(f"‚úÖ ƒê√£ ƒë·ªçc v√† x·ª≠ l√Ω {len(texts)} ƒëo·∫°n vƒÉn t·ª´ {len(all_documents)} t√†i li·ªáu")

    def setup_qa_chain(self):
        """Setup QA chain v·ªõi custom prompt"""
        if self.vectorstore is None:
            print("‚ö†Ô∏è Ch∆∞a c√≥ vector store, s·ª≠ d·ª•ng mode chat th∆∞·ªùng")
            return

        # Custom prompt
        prompt = PromptTemplate(
            input_variables=["context", "question"],
            template=RAG_PROMPT_TEMPLATE
        )

        # T·∫°o retrieval QA chain
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vectorstore.as_retriever(
                search_kwargs={"k": VECTOR_CONFIG["k_retrieved_docs"]}
            ),
            chain_type_kwargs={"prompt": prompt},
            return_source_documents=True
        )

    def chat(self, question):
        """Chat v·ªõi AI"""
        if self.qa_chain:
            # S·ª≠ d·ª•ng RAG
            response = self.qa_chain({"query": question})
            return {
                "answer": response["result"],
                "sources": response["source_documents"]
            }
        else:
            # Chat th∆∞·ªùng
            system_message = f"{CUSTOM_SYSTEM_PROMPT}\n\nC√¢u h·ªèi: {question}"
            response = self.llm.invoke(system_message)
            return {
                "answer": response.content,
                "sources": []
            }

    def run_console_chat(self):
        """Ch·∫°y chat console"""
        print("\nü§ñ MyAI ƒë√£ s·∫µn s√†ng! G√µ 'thoat' ƒë·ªÉ k·∫øt th√∫c.\n")

        while True:
            user_input = input("üë§ B·∫°n: ")

            if user_input.lower() in ['thoat', 'exit', 'quit', 'bye']:
                print("ü§ñ MyAI: T·∫°m bi·ªát! H·∫πn g·∫∑p l·∫°i b·∫°n!")
                break

            try:
                print("üîç ƒêang t√¨m ki·∫øm th√¥ng tin...")
                result = self.chat(user_input)

                print(f"\nü§ñ MyAI: {result['answer']}")

                # Hi·ªÉn th·ªã sources n·∫øu c√≥
                if result['sources']:
                    print(f"\nüìö Ngu·ªìn tham kh·∫£o: {len(result['sources'])} t√†i li·ªáu")

                print("-" * 50)

            except Exception as e:
                print(f"‚ùå C√≥ l·ªói x·∫£y ra: {e}")


if __name__ == "__main__":
    # Kh·ªüi t·∫°o v√† ch·∫°y chatbot
    chatbot = RAGChatbot()
    chatbot.run_console_chat()