import os
import shutil
import streamlit as st
from dotenv import load_dotenv
from langchain_groq import ChatGroq
from langchain_community.document_loaders import TextLoader, PyPDFLoader, Docx2txtLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from config import CUSTOM_SYSTEM_PROMPT, RAG_PROMPT_TEMPLATE, MODEL_CONFIG, VECTOR_CONFIG

load_dotenv()


class WebRAGChatbot:
    def __init__(self):
        self.llm = None
        self.vectorstore = None
        self.qa_chain = None
        self.documents_path = "documents"
        self.vector_path = "vector_store"
        self.setup_directories()
        self.setup_llm()

    def setup_directories(self):
        """T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥"""
        os.makedirs(self.documents_path, exist_ok=True)
        os.makedirs(self.vector_path, exist_ok=True)

    def setup_llm(self):
        """Kh·ªüi t·∫°o LLM"""
        self.llm = ChatGroq(
            groq_api_key=os.getenv("GROQ_API_KEY"),
            **MODEL_CONFIG
        )

    def save_uploaded_file(self, uploaded_file):
        """L∆∞u file ƒë∆∞·ª£c upload"""
        try:
            file_path = os.path.join(self.documents_path, uploaded_file.name)
            with open(file_path, "wb") as f:
                f.write(uploaded_file.getbuffer())
            return file_path, True
        except Exception as e:
            st.error(f"L·ªói l∆∞u file: {e}")
            return None, False

    def load_single_document(self, file_path):
        """Load m·ªôt t√†i li·ªáu"""
        try:
            if file_path.endswith('.txt'):
                loader = TextLoader(file_path, encoding='utf-8')
            elif file_path.endswith('.pdf'):
                loader = PyPDFLoader(file_path)
            elif file_path.endswith('.docx'):
                loader = Docx2txtLoader(file_path)
            else:
                return []

            return loader.load()
        except Exception as e:
            st.error(f"L·ªói ƒë·ªçc file {file_path}: {e}")
            return []

    def process_documents(self, force_reload=False):
        """X·ª≠ l√Ω t·∫•t c·∫£ documents"""
        if not force_reload and self.vectorstore is not None:
            return

        # Load t·∫•t c·∫£ documents
        all_documents = []
        doc_files = [f for f in os.listdir(self.documents_path)
                     if f.endswith(('.txt', '.pdf', '.docx'))]

        if not doc_files:
            st.warning("‚ö†Ô∏è Kh√¥ng c√≥ t√†i li·ªáu n√†o trong th∆∞ m·ª•c documents/")
            return

        for doc_file in doc_files:
            file_path = os.path.join(self.documents_path, doc_file)
            docs = self.load_single_document(file_path)
            all_documents.extend(docs)

        if not all_documents:
            return

        # Split documents
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=VECTOR_CONFIG["chunk_size"],
            chunk_overlap=VECTOR_CONFIG["chunk_overlap"]
        )
        texts = text_splitter.split_documents(all_documents)

        # Create embeddings
        embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2"
        )

        # X√≥a vector store c≈© n·∫øu c√≥
        if os.path.exists(self.vector_path):
            shutil.rmtree(self.vector_path)

        # Create vector store m·ªõi
        self.vectorstore = Chroma.from_documents(
            documents=texts,
            embedding=embeddings,
            persist_directory=self.vector_path
        )

        # Setup QA chain
        self.setup_qa_chain()

        st.success(f"‚úÖ ƒê√£ x·ª≠ l√Ω {len(texts)} ƒëo·∫°n vƒÉn t·ª´ {len(all_documents)} t√†i li·ªáu!")

    def setup_qa_chain(self):
        """Setup QA chain"""
        if self.vectorstore is None:
            return

        prompt = PromptTemplate(
            input_variables=["context", "question"],
            template=RAG_PROMPT_TEMPLATE
        )

        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vectorstore.as_retriever(
                search_kwargs={"k": VECTOR_CONFIG["k_retrieved_docs"]}
            ),
            chain_type_kwargs={"prompt": prompt},
            return_source_documents=True
        )

    def chat(self, question):
        """Chat v·ªõi AI"""
        if self.qa_chain:
            response = self.qa_chain({"query": question})
            return {
                "answer": response["result"],
                "sources": response["source_documents"]
            }
        else:
            system_message = f"{CUSTOM_SYSTEM_PROMPT}\n\nC√¢u h·ªèi: {question}"
            response = self.llm.invoke(system_message)
            return {
                "answer": response.content,
                "sources": []
            }

    def get_document_list(self):
        """L·∫•y danh s√°ch t√†i li·ªáu"""
        if not os.path.exists(self.documents_path):
            return []
        return [f for f in os.listdir(self.documents_path)
                if f.endswith(('.txt', '.pdf', '.docx'))]

    def delete_document(self, filename):
        """X√≥a t√†i li·ªáu"""
        try:
            file_path = os.path.join(self.documents_path, filename)
            if os.path.exists(file_path):
                os.remove(file_path)
                return True
        except Exception as e:
            st.error(f"L·ªói x√≥a file: {e}")
        return False


# Streamlit App
st.set_page_config(
    page_title="MyAI - RAG Chatbot v·ªõi Upload",
    page_icon="ü§ñ",
    layout="wide"
)

# Custom CSS
st.markdown("""
<style>
.main-header {
    background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
    padding: 1rem;
    border-radius: 10px;
    color: white;
    text-align: center;
    margin-bottom: 2rem;
}
.upload-section {
    background-color: #f8f9fa;
    padding: 1rem;
    border-radius: 10px;
    border: 2px dashed #dee2e6;
    margin: 1rem 0;
}
</style>
""", unsafe_allow_html=True)

# Header
st.markdown("""
<div class="main-header">
    <h1>ü§ñ MyAI - Tr·ª£ L√Ω Th√¥ng Minh c·ªßa Lu√¢n</h1>
    <p>Upload t√†i li·ªáu v√† chat v·ªõi AI ngay l·∫≠p t·ª©c!</p>
</div>
""", unsafe_allow_html=True)


# Initialize chatbot
@st.cache_resource
def get_chatbot():
    return WebRAGChatbot()


if "chatbot" not in st.session_state:
    st.session_state.chatbot = get_chatbot()

if "messages" not in st.session_state:
    st.session_state.messages = []
    st.session_state.messages.append({
        "role": "assistant",
        "content": "Xin ch√†o! T√¥i l√† MyAI. H√£y upload t√†i li·ªáu ho·∫∑c h·ªèi t√¥i b·∫•t c·ª© ƒëi·ªÅu g√¨!"
    })

# Sidebar - Document Management
with st.sidebar:
    st.header("üìÅ Qu·∫£n L√Ω T√†i Li·ªáu")

    # File Upload Section
    st.subheader("üì§ Upload T√†i Li·ªáu M·ªõi")
    uploaded_files = st.file_uploader(
        "Ch·ªçn file ƒë·ªÉ upload:",
        accept_multiple_files=True,
        type=['txt', 'pdf', 'docx'],
        help="H·ªó tr·ª£: .txt, .pdf, .docx"
    )

    if uploaded_files:
        success_count = 0
        for uploaded_file in uploaded_files:
            file_path, success = st.session_state.chatbot.save_uploaded_file(uploaded_file)
            if success:
                success_count += 1
                st.success(f"‚úÖ ƒê√£ l∆∞u: {uploaded_file.name}")

        if success_count > 0:
            if st.button("üîÑ X·ª≠ L√Ω T√†i Li·ªáu M·ªõi", type="primary"):
                with st.spinner("ƒêang x·ª≠ l√Ω t√†i li·ªáu..."):
                    st.session_state.chatbot.process_documents(force_reload=True)
                st.rerun()

    st.divider()

    # Document List
    st.subheader("üìö T√†i Li·ªáu Hi·ªán C√≥")
    doc_list = st.session_state.chatbot.get_document_list()

    if doc_list:
        for doc in doc_list:
            col1, col2 = st.columns([3, 1])
            with col1:
                st.text(f"üìÑ {doc}")
            with col2:
                if st.button("üóëÔ∏è", key=f"delete_{doc}", help="X√≥a file"):
                    if st.session_state.chatbot.delete_document(doc):
                        st.success(f"ƒê√£ x√≥a {doc}")
                        st.rerun()

        st.info(f"üí° T·ªïng: {len(doc_list)} t√†i li·ªáu")

        if st.button("üîÑ T·∫£i L·∫°i T√†i Li·ªáu"):
            with st.spinner("ƒêang t·∫£i l·∫°i..."):
                st.session_state.chatbot.process_documents(force_reload=True)
            st.rerun()
    else:
        st.warning("Ch∆∞a c√≥ t√†i li·ªáu n√†o")

    st.divider()

    # Stats
    st.subheader("üìä Th·ªëng K√™")
    if hasattr(st.session_state.chatbot, 'vectorstore') and st.session_state.chatbot.vectorstore:
        try:
            doc_count = st.session_state.chatbot.vectorstore._collection.count()
            st.metric("ƒêo·∫°n vƒÉn ƒë√£ h·ªçc", doc_count)
            st.success("‚úÖ AI ƒë√£ s·∫µn s√†ng!")
        except:
            st.info("üîÑ ƒêang kh·ªüi t·∫°o...")
    else:
        st.warning("‚ö†Ô∏è Ch∆∞a x·ª≠ l√Ω t√†i li·ªáu")

    if st.button("üóëÔ∏è X√≥a L·ªãch S·ª≠ Chat"):
        st.session_state.messages = [st.session_state.messages[0]]
        st.rerun()

# Main Chat Interface
st.subheader("üí¨ Chat v·ªõi MyAI")

# Auto-process documents on first load
if not hasattr(st.session_state, 'docs_processed'):
    if st.session_state.chatbot.get_document_list():
        with st.spinner("üîÑ ƒêang x·ª≠ l√Ω t√†i li·ªáu c√≥ s·∫µn..."):
            st.session_state.chatbot.process_documents()
        st.session_state.docs_processed = True

# Display messages
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])
        if "sources" in message and message["sources"]:
            with st.expander(f"üìö Ngu·ªìn tham kh·∫£o ({len(message['sources'])} t√†i li·ªáu)"):
                for i, source in enumerate(message["sources"], 1):
                    st.write(f"**Ngu·ªìn {i}:**")
                    st.write(source.page_content[:200] + "...")

# Chat input
if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n..."):
    # Add user message
    st.session_state.messages.append({"role": "user", "content": prompt})

    with st.chat_message("user"):
        st.write(prompt)

    # Get AI response
    with st.chat_message("assistant"):
        with st.spinner("ü§î MyAI ƒëang suy nghƒ©..."):
            try:
                result = st.session_state.chatbot.chat(prompt)

                st.write(result["answer"])

                if result["sources"]:
                    with st.expander(f"üìö Ngu·ªìn tham kh·∫£o ({len(result['sources'])} t√†i li·ªáu)"):
                        for i, source in enumerate(result["sources"], 1):
                            st.write(f"**Ngu·ªìn {i}:**")
                            st.write(source.page_content[:200] + "...")

                st.session_state.messages.append({
                    "role": "assistant",
                    "content": result["answer"],
                    "sources": result["sources"]
                })

            except Exception as e:
                error_msg = f"‚ùå C√≥ l·ªói x·∫£y ra: {str(e)}"
                st.error(error_msg)
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": error_msg
                })

# Instructions
with st.expander("üí° H∆∞·ªõng D·∫´n S·ª≠ D·ª•ng"):
    st.markdown("""
    **C√°ch th√™m t√†i li·ªáu:**

    1. **Upload tr·ª±c ti·∫øp:** D√πng file uploader b√™n tr√°i
    2. **Copy file:** Th·∫£ file v√†o th∆∞ m·ª•c `documents/` v√† nh·∫•n "T·∫£i L·∫°i"

    **C√°c ƒë·ªãnh d·∫°ng h·ªó tr·ª£:**
    - üìÑ Text files (.txt)
    - üìï PDF files (.pdf)
    - üìò Word files (.docx)

    **Tips:**
    - Upload xong nh·ªõ nh·∫•n "X·ª≠ L√Ω T√†i Li·ªáu M·ªõi"
    - AI s·∫Ω tr·∫£ l·ªùi d·ª±a tr√™n n·ªôi dung t√†i li·ªáu
    - C√≥ th·ªÉ x√≥a t√†i li·ªáu kh√¥ng c·∫ßn thi·∫øt ƒë·ªÉ tƒÉng ƒë·ªô ch√≠nh x√°c
    """)